{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shopee NDSC 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team : PelajarHsinchu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ðŸ”­ Farhan Tandia [LinkedIn](https://www.linkedin.com/in/mfarhantandia/)\n",
    "- âš¡ Ivan Surya Hutomo [LinkedIn](https://www.linkedin.com/in/ivan-surya-hutomo-b5746713a/)\n",
    "- ðŸŒ± Mahdin Rohmatillah [LinkedIn](https://www.linkedin.com/in/mahdin-rohmatillah-99596799/)\n",
    "- ðŸ¥… Martin Dominikus [LinkedIn](https://www.linkedin.com/in/martindominikus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import all required Dependencies'''\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten, Dropout, MaxPooling2D,Multiply, Dropout, Subtract, Add,  BatchNormalization,Lambda,GlobalAveragePooling2D,GlobalMaxPool2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3, EfficientNetB6\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Concatenate,  Dropout, Conv2D, BatchNormalization, MaxPool2D, Activation, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Training Data'''\n",
    "\n",
    "df = pd.read_csv(\"new_training_set.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Know data distribution between positive and negative values'''\n",
    "\n",
    "neg, pos = np.bincount(df['Label'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert training data from csv to train generator'''\n",
    "\n",
    "datagen=ImageDataGenerator(validation_split = 0.2)\n",
    "train_generator1=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_1\",\n",
    "y_col=\"Label\",\n",
    "subset=\"training\",\n",
    "batch_size=11000,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))\n",
    "\n",
    "train_generator2=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_2\",\n",
    "y_col=\"Label\",\n",
    "subset=\"training\",\n",
    "batch_size=11000,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split training data from csv to validation generator'''\n",
    "\n",
    "\n",
    "valid_generator1=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_1\",\n",
    "y_col=\"Label\",\n",
    "subset=\"validation\",\n",
    "batch_size=2036,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))\n",
    "\n",
    "valid_generator2=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_2\",\n",
    "y_col=\"Label\",\n",
    "subset=\"validation\",\n",
    "batch_size=2036,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract training data and label from generator'''\n",
    "x_train1, y_train = next(train_generator1)\n",
    "x_train2, y_train = next(train_generator2)\n",
    "x_train1 = x_train1.astype('float32')\n",
    "x_train2 = x_train2.astype('float32')\n",
    "y_train = y_train.astype('uint8')\n",
    "input_image_shape = (x_train1.shape[1], x_train1.shape[2], 3)\n",
    "\n",
    "\n",
    "'''Extract validation data and label from generator'''\n",
    "x_valid1, y_val = next(valid_generator1)\n",
    "x_valid2, y_val = next(valid_generator2)\n",
    "x_valid1 = x_valid1.astype('float32')\n",
    "x_valid2 = x_valid2.astype('float32')\n",
    "y_val = y_val.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Add Data Augmentation to training data and validation'''\n",
    "datagen=ImageDataGenerator(zoom_range=0.1,rotation_range=10,validation_split=0.2)\n",
    "\n",
    "train_generator1=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_1\",\n",
    "y_col=\"Label\",\n",
    "subset=\"training\",\n",
    "batch_size=10000,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))\n",
    "\n",
    "train_generator2=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_2\",\n",
    "y_col=\"Label\",\n",
    "subset=\"training\",\n",
    "batch_size=10000,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_generator1=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_1\",\n",
    "y_col=\"Label\",\n",
    "subset=\"validation\",\n",
    "batch_size=2036,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))\n",
    "\n",
    "valid_generator2=datagen.flow_from_dataframe(\n",
    "dataframe=df,\n",
    "directory=\"training_img/training_img\",\n",
    "x_col=\"image_2\",\n",
    "y_col=\"Label\",\n",
    "subset=\"validation\",\n",
    "batch_size=2036,\n",
    "shuffle = False,\n",
    "class_mode='binary',\n",
    "target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train11, y_train1 = next(train_generator1)\n",
    "x_train21, y_train1 = next(train_generator2)\n",
    "x_train11 = x_train11.astype('float32')\n",
    "x_train21 = x_train21.astype('float32')\n",
    "y_train1 = y_train1.astype('uint8')\n",
    "\n",
    "x_valid11, y_val1 = next(valid_generator1)\n",
    "x_valid21, y_val1 = next(valid_generator2)\n",
    "x_valid11 = x_valid11.astype('float32')\n",
    "x_valid21 = x_valid21.astype('float32')\n",
    "y_val1 = y_val1.astype('uint8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Concate original data and augmented data'''\n",
    "xtrain_1 = np.concatenate((x_train1,x_train11),axis=0)\n",
    "xtrain_2 = np.concatenate((x_train2,x_train21),axis=0)\n",
    "y_train = np.concatenate((y_train1,y_train1))\n",
    "\n",
    "xvalid_1 = np.concatenate((x_valid1,x_valid11),axis=0)\n",
    "xvalid_2 = np.concatenate((x_valid2,x_valid21),axis=0)\n",
    "y_val = np.concatenate((y_val1,y_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create siamese network'''\n",
    "\n",
    "def get_siamese_model(input_shape):\n",
    "  \n",
    "    input_1 = Input(input_shape)\n",
    "    input_2 = Input(input_shape)\n",
    "    \n",
    "   \n",
    "    base_model = EfficientNetB6(weights='imagenet', include_top=False)\n",
    "\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "\n",
    "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAveragePooling2D()(x1)])\n",
    "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAveragePooling2D()(x2)])\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x = Concatenate(axis=-1)([x4, x3])\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['AUC'], optimizer=tfa.optimizers.LazyAdam(0.001))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_siamese_model((100, 100, 3))\n",
    "model.load_weights('Checkpoint_NovoGrad_30Epochs_ValAuc9238_27 Nov 2020_Weight.h5') #Load weight from precompetition checkpoint\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Show model architecture image'''\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Start training the model'''\n",
    "\n",
    "filepath = 'best_model_ndsc.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "# stop = EarlyStopping(monitor='val_loss', patience =3,\n",
    "#                       verbose=1, mode='auto', baseline=None, \n",
    "#                       restore_best_weights=False)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit([xtrain_1,xtrain_2], y_train, validation_data=([xvalid_1,xvalid_2], y_val),\n",
    "          epochs=60,shuffle=True,\n",
    "          batch_size=4,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Checkpoint_NovoGrad_90Epochs_ValAuc9238_27 Nov 2020_Weight.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_ndsc.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"new_test_sample.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"sample_img/sample_img/\"\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "prediction=[]\n",
    "for i,j in zip(df_test['image_1'],df_test['image_2']):\n",
    "    img1 = image.load_img(img_path+i, target_size=(100,100))\n",
    "    img_data1 = image.img_to_array(img1)\n",
    "    img_data1 = np.expand_dims(img_data1, axis=0)\n",
    "    img2 = image.load_img(img_path+j, target_size=(100,100))\n",
    "    img_data2 = image.img_to_array(img2)\n",
    "    img_data2 = np.expand_dims(img_data2, axis=0)\n",
    "    img_identity = model.predict([img_data1, img_data2])\n",
    "    \n",
    "    if img_identity >0.6: \n",
    "        img_identity=1\n",
    "    else : img_identity=0\n",
    "        \n",
    "    prediction.append(img_identity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame() \n",
    "df_result['Label']=prediction\n",
    "df_result.index.names = ['pair_index']\n",
    "df_result.to_csv(r'submission_b6.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}